
# LLM Inference Optimization: Quantization for Efficient Large Language Models

## Overview

This project explores state-of-the-art optimization techniques‚Äîspecifically **dynamic quantization** to accelerate inference for large language models (LLMs). By comparing baseline full‚Äëprecision inference against optimized models on an M1 Mac, we aim to reduce latency and memory consumption while preserving output quality. The work demonstrates practical applications of model compression strategies for real-world deployment in resource-constrained environments.

## Motivation

Large language models have achieved remarkable results in NLP but are notorious for high inference costs. This project:
- **Reduces latency:** By compressing model weights and streamlining computations.
- **Decreases memory footprint:** Through lower-precision operations and sparse weight representations.
- **Maintains quality:** Balancing performance with minimal impact on output fidelity.

The work is particularly relevant for deploying AI services on edge devices or platforms with limited resources.

## Key Features

- **Dynamic Quantization Pipeline**: Implements INT8 quantization using PyTorch's `qnnpack` engine, specifically optimized for transformer architectures
- **Automated Benchmarking Suite**: Comprehensive performance analysis across multiple metrics:
  - Inference latency (mean and standard deviation)
  - Memory utilization tracking
  - Token generation throughput
- **Multi-Modal Inference Support**: Optimized for various compute environments:
  - CPU optimization with `qnnpack`
  - Automatic device selection (CPU/MPS)
  - Memory-efficient inference patterns

## Performance Metrics

The framework provides extensive benchmarking capabilities:
- Warm-up run elimination to ensure accurate timing
- Statistical analysis over multiple inference runs (n=20 by default)
- Memory tracking at the process level
- Token generation speed analysis

## Technical Architecture

### Core Components

1. **Baseline Inference Module** (`inference.py`)
```python
def run_baseline_inference(
    model_name="gpt2-large",
    temperature=0.7,
    top_k=30,
    top_p=0.85,
    repetition_penalty=1.2
)
```
- Implements optimized inference parameters
- Automatic device selection (CPU/MPS)
- Memory-efficient execution patterns
- Process-level memory tracking

2. **Quantization Engine** (`quantization.py`)
```python
def apply_dynamic_quantization(model="gpt2-large"):
    return torch.ao.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )
```
- Dynamic quantization of linear layers to INT8
- Selective layer quantization support
- Automated model export and persistence

3. **Benchmarking System** (`benchmark.py`)
```python
def benchmark_inference(
    model,
    tokenizer,
    prompt,
    n_runs=20,
    max_length=150
)
```
- Statistical performance analysis
- Warm-up run execution
- Standard deviation calculation
- Token generation tracking

## üîß Installation & Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/inference-optimization.git

# Navigate to project directory
cd inference-optimization

# Install dependencies
pip install torch transformers psutil
```

## Usage

### Running Baseline Inference

```bash
./scripts/run_baseline.sh
```

### Applying Quantization

```bash
./scripts/run_quantization.sh
```

### Running Benchmarks

```python
python src/benchmark.py
```

## üî¨ Implementation Details

### Optimization Techniques

1. **Dynamic Quantization**
   - INT8 quantization for linear layers
   - Selective layer quantization
   - `qnnpack` engine optimization

2. **Memory Management**
   - Explicit cache clearing for MPS devices
   - Process-level memory tracking
   - Efficient tensor management

3. **Generation Parameters**
   - Optimized temperature (0.7)
   - Balanced top-k (30) and top-p (0.85)
   - Repetition penalty implementation (1.2)

## üìã Project Structure

```
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ quantized_gpt2/
‚îÇ       ‚îî‚îÄ‚îÄ state_dict.pth
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ run_baseline.sh
‚îÇ   ‚îî‚îÄ‚îÄ run_quantization.sh
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ benchmark.py
    ‚îú‚îÄ‚îÄ inference.py
    ‚îú‚îÄ‚îÄ quantization.py
    ‚îî‚îÄ‚îÄ qat_training.py
```

## Technical Dependencies

- PyTorch >= 1.13.0
- Transformers >= 4.25.1
- Python >= 3.8
- QNNPACK backend support

## ü§ù Contributing

Contributions are welcome! Areas of particular interest:
- Additional quantization techniques
- Multi-GPU support
- Extended benchmarking metrics
- Alternative model architecture support
